# -*- coding: utf-8 -*-
"""Predicting the next number of series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qLvCtlkOJeexzyvKOFQis6ftLbP68e0u
"""

import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import LSTM,CuDNNLSTM
from keras.layers import Dense
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# normalise:  to decrease the gradient and make the model to learn easily
data=[[[(i+j)/100] for i in range(5)] for j in range(100)]
target=[(i+5)/100 for i in range(100)]

data=np.array(data,dtype=float)
target=np.array(target,dtype=float)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(data,target,test_size=0.2,random_state=4)

model=Sequential()

model.add(CuDNNLSTM((1),batch_input_shape=(None,5,1),return_sequences=True)) #first layer
model.add(CuDNNLSTM((1),return_sequences=False)) ##second layer
#1 is the o/p
#batch_input_shape=input shape of data(no. of i/p, num of i/p sequences, len of each vector)
#setting to false= only 1 o/p will be given, and thats what we need here. just one o/p

model.compile(loss='mean_absolute_error',optimizer='adam',metrics=['accuracy'])

model.summary()

history=model.fit(x=x_train,y=y_train, epochs=700, verbose=0)
#history will help in visualising loss

model.evaluate(x_test, y_test)

results=model.predict(x_test)

plt.scatter(range(20),results,c='r')
plt.scatter(range(20),y_test,c='g')
plt.show()

plt.plot(history.history['loss'])
plt.show()